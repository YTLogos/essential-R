[
["about.html", "Essential R About", " Essential R Simon Dirmeier 2019-03-05 About The lyf so short, the craft so long to lerne. – Geoffrey Chaucer This book serves as a personal collection of tools for package development, good practices for programming, and most frequently used packages. The material treated here certainly does not cover all of R, but rather serves as list of essential things related to programming in R that I find useful to know. The document is partly opinionated and subjective, so feel free to open up an issue if you feel some parts should be clarified or reformulated. The book is no introduction on how to program functionally, procedurally or in an object-oriented way, how to write code in general, or how to speed it up. The interested reader is referred to: Robert Martin: Clean Code, Andrew Hunt: The Pragmatic Programmer, Gang of Four: Design Patterns, Colin Gillespie: Efficient R programming, Patrick Burns: The R Inferno, Hadley Wickham: R packages, Hadley Wickham: Advanced R, Dirk Eddelbuettel: Seamless R and C++ Integration with Rcpp, Thomas Cormen: Introduction to Algorithms, Dan Gusfield: Algorithms on Strings, Trees and Sequences, Donald Knuth: The Art of Computer Programming, a comment by Peter Norvig, … "],
["r-package-development.html", "1 R package development 1.1 Creating R packages 1.2 Writing R packages 1.3 Debugging 1.4 Testing code 1.5 Documenting code 1.6 Checking code 1.7 Static code analysis 1.8 Code coverage 1.9 Profiling and benchmarking 1.10 Creating a landing page", " 1 R package development checking for file ‘/home/simon/PROJECTS/essential-R/pkg/DESCRIPTION’ ... ✔ checking for file ‘/home/simon/PROJECTS/essential-R/pkg/DESCRIPTION’ ─ preparing ‘newpkg’: checking DESCRIPTION meta-information ... ✔ checking DESCRIPTION meta-information ─ checking for LF line-endings in source and make files and shell scripts ─ checking for empty or unneeded directories ─ building ‘newpkg_0.99.0.tar.gz’ Running /usr/lib/R/bin/R CMD INSTALL \\ /tmp/RtmpIa7ahI/newpkg_0.99.0.tar.gz --install-tests * installing to library ‘/home/simon/R/x86_64-pc-linux-gnu-library/3.5’ - * installing *source* package ‘newpkg’ ... | ** R - ** inst | ** tests - ** byte-compile and prepare package for lazy loading | ** help - *** installing help indices | ** building package indices - ** installing vignettes | ** testing if installed package can be loaded - * DONE (newpkg) | - The following section covers tools that help and speed up developing R 1.1 Creating R packages A minimum R-package stack at least consists of the following packages of tools: yeoman devtools testthat roxygen2 covr lintr usethis If you have yeoman installed, you can use the R-bones generator in order to initialize a complete project. This gives you the following barebone: yo r-bones ls -la pkg total 844 drwxrwxr-x 8 simon simon 4096 Mär 3 17:07 . drwxrwxr-x 12 simon simon 4096 Mär 5 18:45 .. -rw-rw-r-- 1 simon simon 885 Mär 3 17:07 appveyor.yml -rw-rw-r-- 1 simon simon 410 Mär 3 17:07 .codecov.yml -rw-rw-r-- 1 simon simon 548 Mär 3 18:28 DESCRIPTION drwxrwxr-x 4 simon simon 4096 Mär 3 17:07 docs -rw-rw-r-- 1 simon simon 51 Mär 3 17:07 .gitattributes -rw-rw-r-- 1 simon simon 421 Mär 3 17:07 .gitignore drwxrwxr-x 2 simon simon 4096 Mär 3 17:07 inst -rw-rw-r-- 1 simon simon 35141 Mär 3 17:07 LICENSE -rw-rw-r-- 1 simon simon 556 Mär 3 17:07 .lintr drwxrwxr-x 2 simon simon 4096 Mär 3 17:07 man -rw-rw-r-- 1 simon simon 56 Mär 5 18:37 NAMESPACE -rw-rw-r-- 1 simon simon 736082 Mär 3 17:07 newpkg.html -rw-rw-r-- 1 simon simon 354 Mär 3 17:07 newpkg.Rproj drwxrwxr-x 2 simon simon 4096 Mär 3 17:07 R -rw-rw-r-- 1 simon simon 220 Mär 3 17:07 .Rbuildignore -rw-rw-r-- 1 simon simon 556 Mär 3 17:07 README.md drwxrwxr-x 3 simon simon 4096 Mär 3 17:07 tests -rw-rw-r-- 1 simon simon 93 Mär 3 17:07 TODO.md -rw-rw-r-- 1 simon simon 829 Mär 3 17:07 .travis.yml -rw-rw-r-- 1 simon simon 136 Mär 3 17:07 VERSIONS.md drwxrwxr-x 2 simon simon 4096 Mär 3 17:07 vignettes -rw-rw-r-- 1 simon simon 176 Mär 3 17:07 .yo-rc.json 1.2 Writing R packages When creating an R package devtools and covr cover almost any functionality required. The following functions delineate how my typical workflow looks: devtools::create(&quot;pkg&quot;) devtools::use_rcpp() devtools::document() devtools::test() devtools::check_cran() devtools::lint() devtools::run_examples() covr::package_coverage() devtools::install() This basically covers your complete development life cycle. devtools is tremendously useful. If you look for something that helps you write a package, devtools usually has a function for it. For all other things, use usethis. usethis::use_namespace() usethis::use_code_of_conduct() usethis::use_travis() usethis::use_vignette() usethis::use_gpl3_license() 1.3 Debugging TODO 1.4 Testing code Right after creating your package, you should write your first test (yes, really). Testing is essential*for writing good software. The same way as programming languages change your way of thinking, unit tests change your way of writing functions. testthat is probably the best way to go here. Tests are usually put in tests/testthat. You can use: devtools::use_testthat() to create a test suite automatically. A test would look like this: testthat::test_that(&quot;i know my math&quot;, { testthat::expect_equal(g(), 2) }) testthat::test_that(&quot;i know my math&quot;, { testthat::expect_false(&quot;wrong&quot; == &quot;right&quot;) }) Let’s test this. devtools::test(&quot;./pkg&quot;) ✔ | OK F W S | Context ⠏ | 0 | hello ⠋ | 1 | hello ⠙ | 2 | hello ✔ | 2 | hello ══ Results ════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════ OK: 2 Failed: 0 Warnings: 0 Skipped: 0 A function ideally does one task and one task only. Functions with side effects, multiple operations or exceedingly large method body easily introduce bugs. Keep your functions concise! This also simplifies testing, because it is easier to track down a bug in a shorter function. 1.5 Documenting code Having written the first unit test, we can create the actual function and its respective documentation using roxygen2: #&#39; @title Adds 1 and 1 #&#39; #&#39; @description This magnificent function computes the sum of 1 and 1. #&#39; #&#39; @export #&#39; #&#39; @return returns 2 #&#39; #&#39; @examples #&#39; a &lt;- g() #&#39; print(a) g &lt;- function() 1 + 1 Then build the documentation: devtools::document(&quot;./pkg&quot;) Writing NAMESPACE Writing NAMESPACE An excellent help for creating documentation (of S3 and S4) is for instance pckdev or the official vignette. 1.6 Checking code If you want to submit your package to CRAN or Bioconductor certain criteria must be fulfilled. Some of which can be tested by checking or package like this: devtools::check_cran(&quot;./pkg&quot;) This does not test for Bioconductor though. For this you have to install BioCheck manually and call R CMD BiocCheck newpkg*.tar on the command line. 1.7 Static code analysis lintr checks your code for style, syntax error and possible issues. You can also incorporate lintr in your unit tests and let them fail, if lints are discovered. if (requireNamespace(&quot;lintr&quot;, quietly = TRUE)) { test_that(&quot;this is lint free&quot;, { lintr::expect_lint_free() }) } What lintr considers worth reporting can be customized in a .lintr file in your package root directory. Let’s see if our small package is lint free: devtools::lint(&quot;./pkg&quot;) Linting newpkg.... Whoops! TODO add styler etc 1.7.1 Running examples Examples are extremely helpful both for the user and debugging purposes. When we documented our function g we already have an example how to use the function. We can manually call all examples using: devtools::run_examples(&quot;./pkg&quot;) Updating newpkg documentation Writing NAMESPACE Loading newpkg Writing NAMESPACE ── Running 1 example files ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── newpkg ── Loading newpkg &gt; ### Name: g &gt; ### Title: Adds 1 and 1 &gt; ### Aliases: g &gt; &gt; ### ** Examples &gt; &gt; a &lt;- g() &gt; print(a) [1] 2 Loading newpkg 1.8 Code coverage covr lets you check how much of your code is used and tested. If you created a package just run: covr::package_coverage(&quot;./pkg&quot;) newpkg Coverage: 50.00% R/bad_bad_file.R: 0.00% R/newpkg.R: 100.00% Having high code coverage usually correlates with a good testing suite. The more functionality is tested, the larger the code coverage. You can customized reports by adding a file called .codecov.yml to your project’s root: cat ./pkg/.codecov.yml comment: behavior: default layout: header, diff require_changes: false coverage: precision: 2 range: - 70.0 - 100.0 round: down status: changes: false patch: true project: true parsers: gcov: branch_detection: conditional: true loop: true macro: false method: false javascript: enable_partials: false ignore: - &quot;inst/include/&quot; - &quot;inst/&quot; 1.9 Profiling and benchmarking If you think your code runs slowly, you can try to find the bottleneck, for instance, using profvis and microbenchmark. library(&quot;profvis&quot;) library(&quot;ggthemes&quot;) library(&quot;gam&quot;) data(kyphosis) profvis::profvis({ sp &lt;- stats::spline(kyphosis$Age, kyphosis$Number, method = &quot;n&quot;) gm &lt;- gam::gam(Number ~ Age, family = poisson, data = kyphosis) df &lt;- data.frame(X = c(sp$x, kyphosis$Age), Y = c(sp$y, predict(gm, kyphosis)), model = c(rep(&quot;Spline&quot;, length(sp$x)), rep(&quot;GAM&quot;, length(kyphosis$Age)))) g &lt;- ggplot2::ggplot(df) + ggplot2::geom_point(ggplot2::aes(x = X, y = Y, color = model)) + ggthemes::theme_tufte() print(g) }) Often it however suffices to just benchmark two methods against each other. f &lt;- function(n) { sum &lt;- 0 for (i in seq(n)) sum &lt;- sum + i } g &lt;- function(.) sum(.) microbenchmark::microbenchmark(f(10000), g(10000)) Unit: nanoseconds expr min lq mean median uq max neval f(10000) 364866 365822.0 397365.53 367681 375244 2739348 100 g(10000) 391 467.5 10804.23 556 694 1005711 100 1.10 Creating a landing page At this point you are finished writing your package and you want to provide it to a large user base. A nice landing page often helps gaining popularity. The easiest way to do so is using pkgdown. pkgdown::build_site(pkg = &quot;./pkg&quot;) This creates a web-page like this: A good example can be found here. "],
["programming-paradigma-in-r.html", "2 Programming paradigma in R 2.1 Functional programming 2.2 Object-oriented programming", " 2 Programming paradigma in R TODO 2.1 Functional programming TODO purrr is a functional programming toolkit, much like the apply class of functions. However, purrr does so in a more unified way with consistent return values. Furthermore, in combination with magrittr, the code you write is naturally more concise and easier to read. library(&quot;purrr&quot;) library(&quot;ggplot2&quot;) library(&quot;gganimate&quot;) library(&quot;gapminder&quot;) library(&quot;repurrrsive&quot;) library(&quot;magrittr&quot;) library(DiagrammeR) utils::head(sw_people[[1]]) $name [1] &quot;Luke Skywalker&quot; $height [1] &quot;172&quot; $mass [1] &quot;77&quot; $hair_color [1] &quot;blond&quot; $skin_color [1] &quot;fair&quot; $eye_color [1] &quot;blue&quot; df &lt;- map_dfr(sw_people, .f = function(.) data.frame(color = .[[&quot;eye_color&quot;]], height = .[[&quot;height&quot;]], mass = .[[&quot;mass&quot;]])) utils::head(df) color height mass 1 blue 172 77 2 yellow 167 75 3 red 96 32 4 yellow 202 136 5 brown 150 49 6 blue 178 120 2.2 Object-oriented programming R has three different native ways for object-oriented programming and as far as I know one additional library. S3, S4 and refClasses (R5) come with the standard library, while R6 can be installed from CRAN. I actually never use R5, because it feels incredibly bulky and slow, so we will not cover it here. 2.2.1 S3 S3 methods dispatch on the first argument. If you come from other languages, such as Java, an S3 method is basically an overloaded function on the first argument. You can define an S3 function like this: s3 &lt;- function(x, y, ...) UseMethod(&quot;s3&quot;) s3.matrix &lt;- function(x, y, ...) apply(x, 1, sum) s3.character &lt;- function(x, y, ...) paste(x, y) s3(matrix(1:6, 2)) [1] 9 12 s3(&quot;hello&quot;, &quot;reader&quot;) [1] &quot;hello reader&quot; S3 classes are defines like this: s3.list &lt;- function(x, y, ...) { l &lt;- list(x = x, y = y) base::class(l) &lt;- &quot;my.s3&quot; l } s3(list(x = 1), y = 2) $x $x$x [1] 1 $y [1] 2 attr(,&quot;class&quot;) [1] &quot;my.s3&quot; The main issue here is of course that the user can easily overwrite a class and that method dispatching on one argument usually is not enough. However, often S3 functions are all you need. 2.2.2 S4 Bioconductor seems to prefer S4 over S3, so if you want to submit your package, you could for instance define interfaces using S4 and the rest using S3 or w/o OO entirely. In that way the user of your package only sees the exported interface (the S4 method) and upon calling would receive an S4 object. The rest of the implementation would be hidden. methods::setClass(&quot;normallist&quot;, representation = list(.el = &quot;list&quot;), prototype = methods::prototype(.el = list())) methods::setGeneric(&quot;put&quot;, function(obj, x) base::standardGeneric(&quot;insert&quot;)) methods::setMethod(&quot;put&quot;, signature = methods::signature(obj = &quot;normallist&quot;, x = &quot;vector&quot;), function(obj, x) obj@.el &lt;- as.list(x)) d &lt;- methods::new(&quot;normallist&quot;) d &lt;- put(d, seq(3)) 2.2.3 R6 As a more modern, faster alternative to reference classes (not covered here), R6 is a good option. R6 comes with proper encapsulation, are mutable and not copied on modification. In my experience, a statistical language like R does not need much proper object orientation and S3 or S4 suffice as interface methods. Computations as below, where we could for instance construct a graph, are mostly done in C++ or so anyways. node &lt;- R6Class(&quot;node&quot;, list(id = NA_integer_, neighbors = list(), initialize = function(id, neighbors = NULL) { self$id &lt;- id self$neighbors &lt;- neighbors }, print = function(...) { cat(&quot;ID: &quot;, self$id, &quot;\\n&quot;, sep = &quot;&quot;) neighs &lt;- paste(sapply(self$neighbors, function(.) .$id), collapse = &quot;, &quot;, sep = &quot;, &quot;) cat(&quot;\\tneighbors: &quot;, neighs, &quot;\\n&quot;, sep = &quot;&quot;) }, add = function(node) { self$neighbors &lt;- c(self$neighbors, node) node$neighbors &lt;- c(node$neighbors, self) })) n1 &lt;- node$new(1L) n2 &lt;- node$new(2L) n1$add(n2) n1 n2 n1$neighbors[[1]]$id &lt;- 3 n2 "],
["r-extensions.html", "3 R extensions 3.1 C++ and Rcpp 3.2 Fortran 3.3 Reticulate", " 3 R extensions The following section treats how you can interface R with other languages, e.g. C++, Fortran or Python. The first two I use primarily for speeding up numerical code, although using Fortran become somewhat obsolete. I use Python mostly for interfacing machine learning libraries, such as tensorflow, keras or GPFlow. 3.1 C++ and Rcpp Using Rcpp (and RcppEigen, RcppArmadillo, Boost) you can easily extend your code to C++. It not only nicely wraps the standard C API, but also let’s you use standard matrix libraries such as Eigen and Armadillo or Boost. If you have a project that’s not a package you can for instance create a function like this: Rcpp::cppFunction(&quot;double sum(std::vector&lt;double&gt;&amp; vec) { double sum = .0; for (std::vector&lt;double&gt;::size_type i = 0; i &lt; vec.size(); ++i) sum += vec[i]; return sum; }&quot;) sum(as.numeric(seq(10))) [1] 55 Let’s have a look at an Eigen example with sourceCpp: cat ./_src/square.cpp // [[Rcpp::depends(RcppEigen)]] #include &lt;RcppEigen.h&gt; // [[Rcpp::export]] Eigen::MatrixXd square(Eigen::MatrixXd&amp; m) { return m.transpose() * m; } library(&quot;RcppEigen&quot;) Rcpp::sourceCpp(&quot;./_src/square.cpp&quot;) square(matrix(rnorm(10), 2)) [,1] [,2] [,3] [,4] [,5] [1,] 5.8796994 2.4710347 3.587033 2.7327981 0.1372097 [2,] 2.4710347 2.6095616 2.611945 0.8948609 2.0985710 [3,] 3.5870330 2.6119446 2.964747 1.4888955 1.5184331 [4,] 2.7327981 0.8948609 1.488895 1.3111131 -0.2657189 [5,] 0.1372097 2.0985710 1.518433 -0.2657189 2.6544504 Even for small matrices, the speed up is already substantial. m &lt;- matrix(rnorm(100 * 100), 100) microbenchmark::microbenchmark(square(m), t(m) %*% m) Unit: microseconds expr min lq mean median uq max neval square(m) 282.920 299.0500 389.1880 301.6985 320.4470 1509.044 100 t(m) %*% m 85.628 89.6305 352.7502 120.0165 204.7815 4789.519 100 3.1.1 Rcpp in a package Usually, you would want to put your source code into a cpp file in the src folder. Then you can call C++ from an R function, for instance as described below. Define a C++ source file and put it in src, like this: #include &lt;Rcpp.h&gt; // [[Rcpp::export]] Rcpp::List dostuff() { // some computations return Rcpp::List::create( Rcpp::Named(&quot;a&quot;) = ... ); } Add this to your DESCRIPTION: Imports: Rcpp LinkingTo: Rcpp Add a comment #' @useDynLib pkg, .registration = TRUE to the documentation of any function, better yet to the package doc. Call devtools::document() Call Rcpp::compileAttributes(&quot;./pkg&quot;) This should let you access the C++ function from your package. For more details check out the main documentation or some of my packages, like netReg or datastructures. The latter also has an example how to use modules and Boost. 3.1.2 Writing C++ Here are some tools that help you develop your code: gdb and lldb for debugging, valgrind and gprof to check for memory leaks and for profiling, Intel’s Parallel Studio XE (&lt;3) which is a toolbox for vectorization, debugging, memory checks, etc. If you can get hold of it, get it. It is magnificent. Boost for unit tests, data structures and basically everything you ever need, cppcheck for static code analysis, doxygen for code documentation. Many C++ libraries, like Dlib, Eigen or tensorflow have an R interface, so before implementing functionality yourself check out if there is already an implementation for it. At this point it also makes sense to have a look at various C++ books such as Scott Meyers: Effective Modern C++, Scott Meyers: Effective C++, Ben Klemens: 21. century C, Kurt Guntheroth: Optimized C++, Nicolai Josuttis: C++17 - the complete guide, David Vandevoorde: C++ Templates - the compete guide. For fast numerical code OpenMP and advanced vector and streaming SIMD extensions (AVX/SSE) is often the way to go. Writing good code using AVX is however not very simple and knowledge of memory alignment is required. However, starting from version 4 (?), OpenMP easily allows vectorization using #pragma simd. Note: the last lines in this section are heavily subjective and only state the author’s opinion. More and more garbage code is forced onto the scientific community, mostly related to academic hubris and lack of knowledge about how to program* (we don’t mean how to accomplish a task using a programming language, but to embrace a language and its philosophy at its very core). If you aim to publish your code, make sure it follows contemporary good practices, standards and guidelines and that it is consistent, for instance by following people’s code who are part of the community for ages. At this point we want to emphasize that the author doesn’t claim to know C++, but rather that he is annoyed by this very trend.* C++ is a large, complex language. Chances that you mess up, introduce bugs and memory leaks are high, unless you have some experience. In academia in mostly results in bad, un-maintainable code. If your argument is but it is faster than R, at least have a look at Fortran, Julia or Python (and numpy). If you still prefer hacking in C++ try to stick to some guidelines: don’t include C headers in your C++ projects .. just don’t. C and C++ are two languages. There is for need for FILE* or malloc. modern C++ rarely needs pointers. Use std::vector or std::unique_ptr. No need to manually release pointers is an advantage of itself. BOOST has (probably) everything you will ever need. Don’t reinvent the wheel and learn to include libraries in your project. Learn how to use Cmake, Meson or autotools. You should never need to directly invoke a compiler or write a hard-coded Makefile. Only because you have a PhD or M. Sc. in CS does not make you a prolific programmer. It requires staying up to date, reading other people’s code and embracing of new ideas and the very philosophy of the language and its community. Print warnings. They are usually there for a reason. Start a toy project or contribute to OSS projects. 3.2 Fortran If you don’t want to include extra libraries like RcppArmadillo to keep your package small, you can either use the native C API, or, if you want to make use of BLAS and LAPACK, the Fortran API. Fortran has similar memory management as C, but is overall a far easier language to write numerical code with. I recommend this manual for more details. TODO: some examples like above? 3.3 Reticulate If you need to include Python code for analyses and then do, for instance, the plotting and reporting in R, check out reticulate. library(reticulate) For instance we can create random numbers in Python: import numpy mean = [0, 0] cov = [[1, 0], [0, 1]] x = numpy.random.multivariate_normal(mean, cov, 1000) And then plot stuff in R: library(ggplot2) ggplot(as.data.frame(py$x)) + geom_point(aes(V1, V2)) + theme_minimal() "],
["working-with-data.html", "4 Working with data 4.1 tidyverse 4.2 data.table 4.3 Data structures 4.4 Databases", " 4 Working with data Every analysis of data starts with preprocessing and parsing. The following chapter covers some packages that simplify working with data. 4.1 tidyverse library(tidyverse) data(iris) head(iris) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3.0 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5.0 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa dplyr::group_by(iris, Species) %&gt;% dplyr::summarize(Petal.Length = mean(Petal.Length), Sepal.Length = mean(Sepal.Length)) %&gt;% dplyr::mutate(Species = toupper(Species)) %&gt;% head() # A tibble: 3 x 3 Species Petal.Length Sepal.Length &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 SETOSA 1.46 5.01 2 VERSICOLOR 4.26 5.94 3 VIRGINICA 5.55 6.59 tidyr::gather(iris, Col, Val, -Species) %&gt;% head() Species Col Val 1 setosa Sepal.Length 5.1 2 setosa Sepal.Length 4.9 3 setosa Sepal.Length 4.7 4 setosa Sepal.Length 4.6 5 setosa Sepal.Length 5.0 6 setosa Sepal.Length 5.4 TODO some examples 4.2 data.table data.table is a fast implementation of R’s classical data.frame. I hardly ever use data frame any more, and if so only, because it seems to work nicer with dplyr and tidyr. However, by using dtplyr this isn’t much of a problem, really. library(data.table) library(dplyr) library(dtplyr) library(grid) library(gridExtra) n &lt;- 1000 rn &lt;- stats::rnorm(n) ltrs &lt;- base::sample(letters[1:5], n, replace = TRUE) dt &lt;- data.table::data.table(X = rn, Y = ltrs) df &lt;- base::data.frame(X = rn, Y = ltrs) dt[, .SD[sample(.N, 1)], by = c(&quot;Y&quot;)] %&gt;% tableGrob(rows = NULL) %&gt;% grid.arrange In the end it depends what style you prefer. I usually go with data.table alone without needing the dplyr/dtplyr dependency. However, the latter is usually more readable. For large data, the fastest solution is probably preferable. dt.only &lt;- function() dt[, .SD[sample(.N, 1)], by = c(&quot;Y&quot;)] dt.dtplyr &lt;- function() dt %&gt;% dplyr::group_by(Y) %&gt;% dplyr::sample_n(1) df.dplyr &lt;- function() df %&gt;% dplyr::group_by(Y) %&gt;% dplyr::sample_n(1) microbenchmark::microbenchmark(dt.only(), dt.dtplyr(), df.dplyr()) Unit: microseconds expr min lq mean median uq max dt.only() 1414.785 1586.2480 5779.886 1703.112 2298.637 68805.670 dt.dtplyr() 2124.899 2327.8425 41565.427 2641.847 8450.181 3576499.515 df.dplyr() 724.789 841.0025 1082.067 941.272 1057.872 7787.377 neval 100 100 100 4.3 Data structures If you have a background in computer science you may wonder, that R does not have support for advanced data structures such as Fibonacci heaps or hashmaps. datastructures tries to solve this. It uses Rcpp modules to export Boost data structures to R: library(datastructures) q &lt;- datastructures::fibonacci_heap(&quot;integer&quot;) q[1:3] &lt;- list(rnorm(3), 2, rnorm(4)) datastructures::pop(q) $`1` [1] -0.1297980 -0.7222048 0.8822356 datastructures::pop(q) $`2` [1] 2 datastructures::pop(q) $`3` [1] -0.9690185 0.5841185 -0.4897903 0.4729006 datastructures::pop(q) NULL TODO examples 4.4 Databases TODO neo4j, maria, sqltie, mongodb, dbplyr "],
["data-analysis-and-statistical-modeling.html", "5 Data analysis and statistical modeling 5.1 Stan 5.2 greta 5.3 MCMC 5.4 mlR and openML 5.5 Tensorflow 5.6 Keras 5.7 Statistical learning 5.8 Big data analytics 5.9 Others", " 5 Data analysis and statistical modeling The following few sections introduce packages I frequently use for data analysis, modelling and machine learning. 5.1 Stan Stan is a great package for Bayesian modelling and inference. Compared to statistical packages that are fit to one model, such as glmnet or lme4, Stan allows to easily define custom Bayesian models for which posterior distributions are automatically inferred using HMC. A simple liner regression model could look like this: library(rstan) library(lme4) model &lt;- &quot; data { int&lt;lower=1&gt; n; vector[n] x; vector[n] y; } parameters { real beta; real&lt;lower=0&gt; sigma; real alpha; } model { beta ~ normal(0, 5); sigma ~ cauchy(0, 5); y ~ normal(alpha + x * beta, sigma); } &quot; n &lt;- nrow(sleepstudy) x &lt;- sleepstudy$Days y &lt;- sleepstudy$Reaction fit &lt;- stan(model_code = model, data = list(n = n, x = x, y = y), warmup = 100, iter = 1100, chains = 1) SAMPLING FOR MODEL &#39;920bbd45e873f3162b84171e9f3ae19f&#39; NOW (CHAIN 1). Chain 1: Chain 1: Gradient evaluation took 2.6e-05 seconds Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. Chain 1: Adjust your expectations accordingly! Chain 1: Chain 1: Chain 1: WARNING: There aren&#39;t enough warmup iterations to fit the Chain 1: three stages of adaptation as currently configured. Chain 1: Reducing each adaptation stage to 15%/75%/10% of Chain 1: the given number of warmup iterations: Chain 1: init_buffer = 15 Chain 1: adapt_window = 75 Chain 1: term_buffer = 10 Chain 1: Chain 1: Iteration: 1 / 1100 [ 0%] (Warmup) Chain 1: Iteration: 101 / 1100 [ 9%] (Sampling) Chain 1: Iteration: 210 / 1100 [ 19%] (Sampling) Chain 1: Iteration: 320 / 1100 [ 29%] (Sampling) Chain 1: Iteration: 430 / 1100 [ 39%] (Sampling) Chain 1: Iteration: 540 / 1100 [ 49%] (Sampling) Chain 1: Iteration: 650 / 1100 [ 59%] (Sampling) Chain 1: Iteration: 760 / 1100 [ 69%] (Sampling) Chain 1: Iteration: 870 / 1100 [ 79%] (Sampling) Chain 1: Iteration: 980 / 1100 [ 89%] (Sampling) Chain 1: Iteration: 1090 / 1100 [ 99%] (Sampling) Chain 1: Iteration: 1100 / 1100 [100%] (Sampling) Chain 1: Chain 1: Elapsed Time: 0.058223 seconds (Warm-up) Chain 1: 0.862701 seconds (Sampling) Chain 1: 0.920924 seconds (Total) Chain 1: summary(fit) $summary mean se_mean sd 2.5% 25% 50% beta 9.734508 0.10430790 1.307173 7.374048 8.84992 9.669498 sigma 47.782422 0.08083895 2.579821 43.259332 45.99257 47.563146 alpha 254.916993 0.64767282 7.282857 241.316674 250.44446 254.898783 lp__ -789.154520 0.12228032 1.526152 -792.932606 -789.76729 -788.763406 75% 97.5% n_eff Rhat beta 10.55408 12.47037 157.0477 1.024087 sigma 49.38598 53.29628 1018.4457 1.000171 alpha 260.02287 268.50520 126.4422 1.023848 lp__ -788.10495 -787.54819 155.7695 1.008462 $c_summary , , chains = chain:1 stats parameter mean sd 2.5% 25% 50% beta 9.734508 1.307173 7.374048 8.84992 9.669498 sigma 47.782422 2.579821 43.259332 45.99257 47.563146 alpha 254.916993 7.282857 241.316674 250.44446 254.898783 lp__ -789.154520 1.526152 -792.932606 -789.76729 -788.763406 stats parameter 75% 97.5% beta 10.55408 12.47037 sigma 49.38598 53.29628 alpha 260.02287 268.50520 lp__ -788.10495 -787.54819 5.1.1 Rstanarm rstanarm is a package for applied Bayesian modelling that wraps around Stan for easier usage. library(rstanarm) fit.arm &lt;- rstanarm::stan_glm(Reaction ~ Days, sleepstudy, chains = 1, iter = 1100, warmup = 100, family = gaussian()) SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1). Chain 1: Chain 1: Gradient evaluation took 3.5e-05 seconds Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.35 seconds. Chain 1: Adjust your expectations accordingly! Chain 1: Chain 1: Chain 1: WARNING: There aren&#39;t enough warmup iterations to fit the Chain 1: three stages of adaptation as currently configured. Chain 1: Reducing each adaptation stage to 15%/75%/10% of Chain 1: the given number of warmup iterations: Chain 1: init_buffer = 15 Chain 1: adapt_window = 75 Chain 1: term_buffer = 10 Chain 1: Chain 1: Iteration: 1 / 1100 [ 0%] (Warmup) Chain 1: Iteration: 101 / 1100 [ 9%] (Sampling) Chain 1: Iteration: 210 / 1100 [ 19%] (Sampling) Chain 1: Iteration: 320 / 1100 [ 29%] (Sampling) Chain 1: Iteration: 430 / 1100 [ 39%] (Sampling) Chain 1: Iteration: 540 / 1100 [ 49%] (Sampling) Chain 1: Iteration: 650 / 1100 [ 59%] (Sampling) Chain 1: Iteration: 760 / 1100 [ 69%] (Sampling) Chain 1: Iteration: 870 / 1100 [ 79%] (Sampling) Chain 1: Iteration: 980 / 1100 [ 89%] (Sampling) Chain 1: Iteration: 1090 / 1100 [ 99%] (Sampling) Chain 1: Iteration: 1100 / 1100 [100%] (Sampling) Chain 1: Chain 1: Elapsed Time: 0.055629 seconds (Warm-up) Chain 1: 0.510607 seconds (Sampling) Chain 1: 0.566236 seconds (Total) Chain 1: summary(fit.arm) Model Info: function: stan_glm family: gaussian [identity] formula: Reaction ~ Days algorithm: sampling priors: see help(&#39;prior_summary&#39;) sample: 1000 (posterior sample size) observations: 180 predictors: 2 Estimates: mean sd 2.5% 25% 50% 75% 97.5% (Intercept) 247.5 17.4 186.7 245.8 251.1 256.1 264.5 Days 10.4 1.4 7.6 9.6 10.5 11.3 13.2 sigma 49.9 8.6 42.8 46.2 48.0 49.9 78.6 mean_PPD 294.2 16.3 236.3 294.3 297.9 301.4 307.6 log-posterior -967.0 22.7 -1052.3 -961.9 -960.8 -960.2 -959.6 Diagnostics: mcse Rhat n_eff (Intercept) 4.2 1.1 17 Days 0.1 1.0 723 sigma 2.0 1.1 18 mean_PPD 4.1 1.1 16 log-posterior 5.9 1.1 15 For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1). 5.1.2 brms For non-linear multi-level models brms is also a great option. library(brms) fit.multilevel &lt;- brm(Reaction ~ Days + (Days | Subject), sleepstudy, chains = 1, iter = 1000) SAMPLING FOR MODEL &#39;45335b867c1a5a0373bc06c1740c990e&#39; NOW (CHAIN 1). Chain 1: Chain 1: Gradient evaluation took 5.1e-05 seconds Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.51 seconds. Chain 1: Adjust your expectations accordingly! Chain 1: Chain 1: Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) Chain 1: Chain 1: Elapsed Time: 2.43316 seconds (Warm-up) Chain 1: 0.690034 seconds (Sampling) Chain 1: 3.1232 seconds (Total) Chain 1: summary(fit.multilevel) Family: gaussian Links: mu = identity; sigma = identity Formula: Reaction ~ Days + (Days | Subject) Data: sleepstudy (Number of observations: 180) Samples: 1 chains, each with iter = 1000; warmup = 500; thin = 1; total post-warmup samples = 500 Group-Level Effects: ~Subject (Number of levels: 18) Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat sd(Intercept) 26.27 7.17 14.76 42.46 195 1.01 sd(Days) 6.43 1.42 4.13 9.52 126 1.00 cor(Intercept,Days) 0.08 0.29 -0.40 0.69 157 1.00 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat Intercept 251.95 7.40 239.74 266.92 266 1.00 Days 10.29 1.80 6.83 14.27 173 1.01 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat sigma 26.05 1.61 23.31 29.44 252 1.00 Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). 5.1.3 bayesplot For plotting of Bayesian models inferred using the tools mentioned above, you probably want to use bayesplot. For instance, to compare posterior predictive intervals between the rstanarm linear model and the mixed model form brms: library(bayesplot) library(cowplot) p1 &lt;- ppc_intervals(y = sleepstudy$Reaction, yrep = posterior_predict(fit.arm)) + labs(title = &quot;Linear model&quot;) p2 &lt;- ppc_intervals(y = sleepstudy$Reaction, yrep = posterior_predict(fit.multilevel)) + labs(title = &quot;Linear mixed model&quot;) cowplot::plot_grid(p1, p2, ncol = 2, align = &quot;h&quot;) 5.2 greta Model definitions in Stan are arguable difficult in the beginning. As an alternative with greta is not only easier to compose models, but also often faster owing to the fact that it’s developed against tensorflow. Installation can be a bit tedious. For greta version v.0.3 I would install tensorflow from the command line: conda create -y -n r-tensorflow python=3.6 conda install tensorflow==1.10.0 pip install tensorflow-probability==0.30.0 The same model we used for stan above, looks like this with greta: library(greta) library(lme4) n &lt;- nrow(sleepstudy) x &lt;- sleepstudy$Days y &lt;- sleepstudy$Reaction alpha &lt;- variable() beta &lt;- greta::normal(0, 5) sigma &lt;- greta::cauchy(0, 5, truncation = c(0, Inf)) y &lt;- greta::as_data(y) greta::distribution(y) &lt;- greta::normal(alpha + x * beta, sigma) mod &lt;- greta::model(beta) samples &lt;- greta::mcmc(mod, n_samples = 1000, warmup = 100, chains = 1) 5.3 MCMC There are a couple of packages on CRAN especially for Markov Chain Monte Carlo and Bayesian methods, some of which are mentioned below. coda is a package for analysis and diagnostics of MCMC chains. Mostly it takes arguments of class mcmc.list, so put your results into an object of it to be able to use coda: library(coda) coda::autocorr(As.mcmc.list(fit)) [[1]] , , beta beta sigma alpha lp__ Lag 0 1.00000000 -0.09098018 -0.84679910 0.02323685 Lag 1 0.64649460 -0.05659510 -0.65854924 -0.02781911 Lag 5 0.19644228 -0.04945911 -0.24252697 -0.01379953 Lag 10 0.04554359 -0.03235870 -0.09980014 0.05001408 Lag 50 0.04263937 0.04612791 -0.04150179 0.01005839 , , sigma beta sigma alpha lp__ Lag 0 -0.09098018 1.00000000 0.064407207 -0.16797674 Lag 1 -0.03181738 -0.09358071 0.042213847 -0.07805589 Lag 5 -0.02528513 0.02386571 0.023106123 -0.07835147 Lag 10 0.02170223 -0.01175329 -0.001123165 -0.04344650 Lag 50 0.04590280 -0.07346226 -0.029463816 -0.02172717 , , alpha beta sigma alpha lp__ Lag 0 -0.846799096 0.06440721 1.000000000 -0.03091263 Lag 1 -0.644357194 0.05412907 0.758570269 0.01090853 Lag 5 -0.253959966 0.06250224 0.288738733 0.03339915 Lag 10 -0.001105280 0.01588829 0.072123141 -0.04399030 Lag 50 0.007197419 -0.05324803 -0.008289024 -0.01622294 , , lp__ beta sigma alpha lp__ Lag 0 0.023236851 -0.167976737 -0.030912631 1.00000000 Lag 1 0.003303205 -0.074906248 0.003060961 0.59562131 Lag 5 -0.013921540 0.021347125 0.022159315 0.22428970 Lag 10 -0.036899347 0.053249233 0.058175966 0.10776711 Lag 50 -0.052465458 0.009090258 0.050878108 0.04968255 coda::traceplot(As.mcmc.list(fit)) MCMCpack offer an alternative to rstanarm for applied Bayesian modelling. It also comes with a set of useful distributions used frequently in Bayesian modelling, such as the Dirichlet, inverse gamma, etc. library(MCMCpack) invg &lt;- MCMCpack::rinvgamma(1000, 5, 1) hist(invg, breaks = 50, xlab = &quot;X&quot;, main = &quot;&quot;, col = &quot;darkgrey&quot;, family = &quot;serif&quot;) For plotting mcmc.list objects (from coda) MCMCvis is great: MCMCvis::MCMCplot(As.mcmc.list(fit)) 5.4 mlR and openML CRAN offers dozens of packages related to machine and statistical learning, many of which doing the same. mlR wraps many of these into one big library. mlR integrates with openML, an open machine learning platform where people share code, data and algorithms. Here we show an example where we use a Gaussian process to predict the Kyphosis label from the gam package. library(mlr) task &lt;- mlr::makeClassifTask(data = kyphosis, target = &quot;Kyphosis&quot;) lrn &lt;- mlr::makeLearner(&quot;classif.gausspr&quot;) n &lt;- nrow(kyphosis) train.set &lt;- sample(n, size = 2/3 * n) test.set &lt;- setdiff(1:n, train.set) model &lt;- mlr::train(lrn, task, subset = train.set) Using automatic sigma estimation (sigest) for RBF or laplace kernel pred &lt;- stats::predict(model, task = task, subset = test.set) performance(pred, measures = list(mmce, acc)) mmce acc 0.1111111 0.8888889 5.5 Tensorflow Thanks to Rstudio, R users are able to use tensorflow, a library for high performance numerical computations (which for instance greta uses). For instance, a linear model could look like this: library(tensorflow) n &lt;- nrow(sleepstudy) x &lt;- sleepstudy$Days y &lt;- sleepstudy$Reaction # define model beta &lt;- tf$Variable(tf$random_normal(shape(1L), 0, 10)) alpha &lt;- tf$Variable(tf$zeros(shape(1L))) y.hat &lt;- alpha + beta * x # Minimize the mean squared errors. loss &lt;- tf$reduce_mean((y - y.hat)^2) optimizer &lt;- tf$train$GradientDescentOptimizer(0.5) train &lt;- optimizer$minimize(loss) # Launch the graph and initialize the variables. sess &lt;- tf$Session() sess$run(tf$global_variables_initializer()) for (step in 1:100) { sess$run(train) } 5.6 Keras Keras is an interface to popular numerical libraries such as tensorflow and theano for which model/network definitions are independent of the library on the backend. Our tensorflow model from above would look the following in keras: library(keras) model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 1, activation = &quot;linear&quot;, input_shape = 1) model %&gt;% compile(loss = &quot;mse&quot;, optimizer = optimizer_sgd(0.5), metrics = list(&quot;mean_absolute_error&quot;)) 5.7 Statistical learning Some packages for regression: glmnet for \\(\\ell_1\\)- and \\(\\ell_2\\)-penalized linear regression models, lme4 for frequentist mixed models, mgcv for generalized additive models, netReg for graph regularized linear models, xgboost and gbm for boosting. h20 for general machine learning algorithms, TODO some examples 5.8 Big data analytics For Big data analytics I recommend Rstudio’s sparklyr since it nicelt itegrates with the other methods from the tidyverse. For instance, following an example from Rstudio’s tutorials: library(sparklyr) sc &lt;- spark_connect(master = &quot;local&quot;) kmeans_model &lt;- copy_to(sc, iris, &quot;iris&quot;, overwrite = TRUE) %&gt;% select(Petal_Width, Petal_Length) %&gt;% ml_kmeans(centers = 3) 5.9 Others Some other great packages for various data-related things: 5.9.1 modelr modelr defines multiple helper functions related to statistical modelling: library(modelr) sleepstudy %&gt;% modelr::fit_with(lm, modelr::formulas(~Reaction, no_intercept = ~0 + Days, intercept = ~1 + Days)) $no_intercept Call: .f(formula = Reaction ~ 0 + Days, data = data) Coefficients: Days 50.16 $intercept Call: .f(formula = Reaction ~ 1 + Days, data = data) Coefficients: (Intercept) Days 251.41 10.47 5.9.2 kernlab Use kernlab for tasks related to kernels, such as Gaussian process regression or merely computing a Gram-matrix. library(kernlab) x &lt;- matrix(rnorm(25), 5) rbf &lt;- kernlab::rbfdot() (K &lt;- kernlab::kernelMatrix(rbf, x)) An object of class &quot;kernelMatrix&quot; [,1] [,2] [,3] [,4] [,5] [1,] 1.000000e+00 2.538052e-04 3.908063e-08 7.738091e-04 1.235612e-04 [2,] 2.538052e-04 1.000000e+00 2.785052e-10 1.300262e-07 8.685670e-02 [3,] 3.908063e-08 2.785052e-10 1.000000e+00 4.426158e-16 1.270126e-14 [4,] 7.738091e-04 1.300262e-07 4.426158e-16 1.000000e+00 2.306205e-07 [5,] 1.235612e-04 8.685670e-02 1.270126e-14 2.306205e-07 1.000000e+00 x &lt;- sort(rnorm(100)) y &lt;- 3 + 0.5 * x^2 + 1 * x + rnorm(100, 0, 0.05) gpr &lt;- kernlab::gausspr(x, y) Using automatic sigma estimation (sigest) for RBF or laplace kernel plot(x, y, family = &quot;serif&quot;) lines(x, predict(gpr, x), col = &quot;red&quot;) "],
["optimization.html", "6 Optimization 6.1 Convex optimization 6.2 Non-linear optimizaton", " 6 Optimization Much of statistical and machine learning boils down to function optimization (if you are not necessarily interested in Bayesian inefrence). In you don’t want to implement an optimizer for yourself, the following couple of libraries might be of interest to you. 6.1 Convex optimization CVXR is a package for discplined convex optimization. As log as you can formulate your objective following the conventions from the package, CVXR automatically verifies convexity and chooses a solver. As an example, below we compare linear regression models solved using OLS and Huber loss: library(CVXR) library(lme4) x &lt;- rnorm(100) y &lt;- 1 + 2 * x + rnorm(100) beta &lt;- CVXR::Variable(1) obj.ols &lt;- CVXR::sum_squares(y - x %*% beta) result.ols &lt;- solve(CVXR::Problem(CVXR::Minimize(obj.ols))) obj.huber &lt;- CVXR::sum_entries(CVXR::huber(y - x %*% beta, 1)) result.huber &lt;- solve(CVXR::Problem(CVXR::Minimize(obj.huber))) beta.hat.ols &lt;- result.ols$getValue(beta) beta.hat.huber &lt;- result.huber$getValue(beta) beta.hat.ols [1] 2.065401 beta.hat.huber [1] 1.986337 TODO more examples and references 6.2 Non-linear optimizaton In cases, where your objective is not differentiable, you have box-constraints, or whatsoever nloptr is an excellent choice for function minimization. Following the example from above: library(nloptr) ols &lt;- function(x, m, n) { 0.5 * sum((m - n * x)^2) } huaba &lt;- function(x, m, n) { thresh &lt;- sum(abs(m - n * x)) if (thresh &lt;= 1) ols(x, m, n) else thresh - 0.5 } result.hat.ols &lt;- nloptr::nloptr(1, ols, lb = -10, ub = 10, opts = list(algorithm = &quot;NLOPT_LN_SBPLX&quot;, maxeval = 1000), m = y, n = x) result.hat.huaba &lt;- nloptr::nloptr(1, huaba, lb = -10, ub = 10, opts = list(algorithm = &quot;NLOPT_LN_SBPLX&quot;, maxeval = 1000), m = y, n = x) result.hat.ols$solution [1] 2.065407 result.hat.huaba$solution [1] 2.011047 TODO more examples and references "],
["plotting-reporting-and-visualizing.html", "7 Plotting, reporting and visualizing 7.1 Layout 7.2 Publication ready plots 7.3 Colors 7.4 Interactive and animated plots 7.5 Graphs 7.6 RMarkdown 7.7 Shiny 7.8 Others", " 7 Plotting, reporting and visualizing One of the most significant reasons to use R is probably its plotting capabilities, people contributing to ggplot2 and the community’s effort of adding package that integrate with it. Below you’ll find some packages for plotting. 7.1 Layout Use hrbrthemes for a nicer layout: library(hrbrthemes) library(ggsci) import_roboto_condensed() g &lt;- ggplot(df) + geom_point(aes(x = height, y = mass, color = color)) + hrbrthemes::theme_ipsum_rc() + ggsci::scale_color_rickandmorty() print(g) Recently I started changing to simpler layouts and themes, for instance as described on Tufte in R. ggthemes offers some nice options to do so. For instance, if you are feeling jealous that you cannot draft some Excel plots, cause you are working with R: library(ggthemes) g &lt;- ggplot(df) + geom_point(aes(x = height, y = mass), color = &quot;green&quot;) + ggthemes::theme_excel() print(g) 7.2 Publication ready plots cowplot and ggpubr are great for greating publication ready plots: p1 &lt;- ggplot(mtcars, aes(hp, disp)) + geom_point() + ggthemes::theme_tufte() p2 &lt;- ggplot(mtcars, aes(hp, disp)) + geom_point() + ggthemes::theme_gdocs() cowplot::plot_grid(p1, p2, ncol = 2, align = &quot;vh&quot;, labels = c(&quot;Nice&quot;, &quot;Meh&quot;)) If these two are still not enough, I usually go with patchwork: library(patchwork) g + (p1 + p2) + plot_layout(ncol = 1) 7.3 Colors I use colorspace and colorblindr in order to remove some hue and chroma. viridis is a wonderful set of colors for continuous, sequential data. For discrete, qualitative data I mainly use ggthemr and the fresh colors. ggsci also has some wonderful color palettes. scales lets you have a look at a color palette easily. library(colorspace) library(colorblindr) library(viridis) library(ggthemr) library(scales) library(cowplot) ggthemr::ggthemr(&quot;fresh&quot;, &quot;scientific&quot;, spacing = 2, type = &quot;inner&quot;) p1 &lt;- colorblindr::gg_color_gradient() + colorspace::scale_fill_continuous_sequential(&quot;viridis&quot;, c1 = 20, c2 = 70, l1 = 25, l2 = 100) p2 &lt;- colorblindr::gg_color_gradient() + colorspace::scale_fill_continuous_sequential(&quot;Blues&quot;, c1 = 20, c2 = 70, l1 = 25, l2 = 100) p3 &lt;- colorblindr::gg_color_gradient() + colorspace::scale_fill_continuous_diverging(c1 = 40) df &lt;- data.frame(Col = ggthemr::swatch()[1:10], X = 1, Y = seq(10)) p4 &lt;- ggplot(df) + geom_tile(aes(x = Y, y = X), fill = df$Col) + theme_void() cowplot::plot_grid(p1, p2, p3, p4, ncol = 2, align = &quot;vh&quot;) Another great tool is swatches: library(swatches) omega_nebula &lt;- read_ase(system.file(&quot;palettes&quot;, &quot;omega_nebula.ase&quot;, package = &quot;swatches&quot;)) show_palette(omega_nebula) 7.4 Interactive and animated plots Often you want to create an interactive plot, for instance when serving on a shiny instance. One way to do that is using plotly: library(plotly) q &lt;- qplot(data = iris, x = Sepal.Length, y = Sepal.Width, color = Species) + ggthemes::theme_tufte() plotly::ggplotly(q) Recently I also stumbled upon highcharter and ggvis: library(highcharter) hchart(iris, &quot;scatter&quot;, hcaes(x = Sepal.Length, y = Sepal.Width, group = Species)) %&gt;% hc_add_theme(hc_theme_tufte()) library(ggvis) data(&quot;mtcars&quot;) mtcars %&gt;% ggvis(~wt, ~mpg, `:=`(size, input_slider(10, 100)), `:=`(opacity, input_slider(0, 1))) %&gt;% layer_points() Renderer: SVG | Canvas Download With gganimate you can directly create GIFs from you plots: library(gganimate) library(gapminder) g &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, frame = year)) + geom_point() + geom_smooth(aes(group = year), method = &quot;lm&quot;, show.legend = FALSE) + facet_wrap(~continent, scales = &quot;free&quot;) + ggthemes::theme_tufte() gganimate(g) 7.5 Graphs There are many wonderful graph and network libraries around for plotting. For HTML DiagrammeR produces beautiful graphs: library(DiagrammeR) create_graph() %&gt;% add_node(label = expression(Z), node_aes = node_aes(penwidth = 2, fontname = &quot;Arial Narrow&quot;, fontcolor = &quot;black&quot;, fillcolor = &quot;white&quot;, color = &quot;black&quot;)) %&gt;% add_node(label = &quot;X&quot;, node_aes = node_aes(penwidth = 2, fontname = &quot;Arial Narrow&quot;, fontcolor = &quot;black&quot;, fillcolor = &quot;grey&quot;, color = &quot;black&quot;)) %&gt;% add_edge(from = 1, to = 2, edge_aes = edge_aes(color = &quot;black&quot;)) %&gt;% render_graph(layout = &quot;tree&quot;) For PDFs I primarily use igraph (which you can also use for graph algorithms). library(igraph) set.seed(1) g &lt;- igraph::random.graph.game(5, p.or.m = 0.4) l &lt;- igraph::layout.reingold.tilford(g) l[1, 1:2] &lt;- c(1, 1) l[2, 1:2] &lt;- c(0, 0) l[3, 1:2] &lt;- c(1, 0) l[4, 1:2] &lt;- c(3, 0) # l[5, 1:2] &lt;- c(2, 2) plot(g, vertex.size = 9, vertex.color = &quot;black&quot;, vertex.label.degree = 0, vertex.label.cex = 1, vertex.label.dist = 1, vertex.label.color = &quot;black&quot;, edge.color = &quot;darkgrey&quot;, edge.width = 0.5, edge.arrow.size = 0.65, layout = l) Other packages for graphi visualisations include ggnetwork/ggnet, tidygraph, or ggraph. 7.6 RMarkdown Rmarkdown is a great way for documentation, reporting and working reproducibly. Rstudio provides tons of different output formats like web sites (like this one), Tufte style documents, blogs and others. For presentations that use R code I use either xaringan, Slidify or reveal.js. Find all the output formats by Rstudio here. 7.7 Shiny If you want to present your work interactively you can do so by building a web page using shiny. Setting up a Shiny instance for reporting is a great way to present data using interactive plots. Setting up shiny is fairly easy. You need to create a server.R file and ui.R file, e.g. like this: cat R/ui.R library(shiny) library(shinyjs) library(plotly) shinyUI( fluidPage( useShinyjs(), fluidRow( column( width = 6,offset=2, h3(&quot;Scatterplot&quot;), HTML(&quot;&lt;h5&gt;&lt;i&gt;Created using ggplot2, hrbrthemes and ggsci on the iris data.&lt;/i&gt;&lt;/h5&gt;&quot;), plotOutput(&quot;scatterplot&quot;, width=800) ) ) ) ) cat R/server.R library(shiny) library(shinyjs) library(ggplot2) library(ggsci) library(hrbrthemes) shinyServer(function(input, output) { output$scatterplot &lt;- renderPlot({ ggplot(iris, aes(x=Sepal.Width, y=Sepal.Length, colour=Species)) + geom_point(aes(colour=Species)) + geom_smooth(alpha=.1) + hrbrthemes::theme_ipsum_rc() + ggsci::scale_colour_rickandmorty() + labs(title=&quot;Iris scatter plot&quot;, subtitle=&quot;Visualizing sepal width vs. sepal length for several species in a schwifty theme.&quot;) }) }) You can publish your server on shinyapps.io so that it is accessible by everyone. 7.8 Others TODO add some other packages ggraph, ggnet and ggnetwork gganimate tweenr ggpixel animation lattice magick imager TODO add add beautified base plots (tufte or so) add colorschemes magick and imagr scico tidygraph r2d3 "],
["miscellaneous.html", "8 Miscellaneous 8.1 Workflow management 8.2 Pry back the covers of R 8.3 Tidy evaluation 8.4 Compile 8.5 Others", " 8 Miscellaneous The following chapter covers miscellaneous, useful tools and libraries for working with R some of which belong to my favourites. 8.1 Workflow management I recently came across drake as a workflow manager. It allows you to run a project and easily update output files. 8.2 Pry back the covers of R In some cases it is interesting to have a look at the implementation of specific functions, get their sizes in byte or get the current memory consumption. It is especially useful to keep track of the addresses your objects point to. When I was new to R I found it confusing when a reference is dropped and when a new copy of an object is created. library(pryr) pryr::inspect(list()) &lt;VECSXP 0x2d50b428&gt; pryr::inspect(vector()) &lt;LGLSXP 0x2d586c90&gt; pryr::object_size(numeric()) 48 B pryr::object_size(numeric(1)) 56 B x &lt;- stats::rnorm(10) y &lt;- x pryr::address(x) [1] &quot;0x2df28858&quot; pryr::address(y) [1] &quot;0x2df28858&quot; y[1] &lt;- 1 pryr::address(y) [1] &quot;0x2df67f88&quot; 8.3 Tidy evaluation TODO rlang 8.4 Compile Base R can be frustratingly slow at times, especially when you cannot vectorize your code or don’t want to extend it ro C++ or Fortran. Using compiler you can however improve your code a little: library(compiler) jit &lt;- enableJIT(0) slow.mean &lt;- function(x) { sum &lt;- 0 n &lt;- length(x) for (i in 1:n) sum &lt;- sum + x[i] sum/n } fast.mean &lt;- compiler::cmpfun(slow.mean) array &lt;- rnorm(1000) microbenchmark::microbenchmark(slow.mean(array), fast.mean(array), mean(array)) Unit: microseconds expr min lq mean median uq slow.mean(array) 292.833 298.9350 17261.85638 306.7190 316.3315 fast.mean(array) 50.753 51.4015 58.66360 51.7805 53.0555 mean(array) 9.755 12.5225 23.38276 15.4740 17.6665 max neval 1694225.411 100 205.735 100 805.857 100 enableJIT(jit) [1] 0 8.5 Others In case you ever need to use Google’s V8 JavaScript enginge, it has R bindings thanks to Jeroen Ooms. Understanding R on the backend has its benefits of its own. A fantastic package for this is lobstr. "],
["good-practices.html", "9 Good practices 9.1 Continuous integration 9.2 Version control 9.3 Docker 9.4 Code style 9.5 Debugging C++ from R", " 9 Good practices Continuous integration, version control and containerization are three of the many tools of a developer. Here, I quickly introduce how the three can be used for R. 9.1 Continuous integration To be honest, doing all the steps from the section above is tedious and annoying. For that reason I set up Travis CI to take care of running our tests, static analysis and code coverage in the project. Travis works for Mac and Unix. cat ./pkg/.travis.yml language: r sudo: required dist: trusty cache: packages matrix: include: - compiler: gcc addons: apt: sources: - ubuntu-toolchain-r-test packages: - g++-5 env: COMPILER=g++-5 - compiler: clang addons: apt: sources: - ubuntu-toolchain-r-test - llvm-toolchain-precise-3.7 packages: - clang-3.7 env: COMPILER=clang++-3.7 env: global: - R_BUILD_ARGS=&quot;--no-build-vignettes --no-manual&quot; - R_CHECK_ARGS=&quot;--no-build-vignettes --no-manual --as-cran&quot; - LINTR_COMMENT_BOT=false before_install: - cd newpkg r_packages: - covr - testthat - lintr after_script: - tar -C .. -xf $PKG_TARBALL - Rscript -e &#39;covr::codecov()&#39; - Rscript -e &#39;lintr::lint_package()&#39; Check out Travis’ docs for more info. What we are basically telling Travis to do is to check our package --as-cran, run the unit tests, do the code coverage and finally do a static code analysis. In order to do the same for Windows machines, we also use AppVeyor. cat ./pkg/appveyor.yml # DO NOT CHANGE the &quot;init&quot; and &quot;install&quot; sections below # Download script file from GitHub init: ps: | $ErrorActionPreference = &quot;Stop&quot; Invoke-WebRequest http://raw.github.com/krlmlr/r-appveyor/master/scripts/appveyor-tool.ps1 -OutFile &quot;..\\appveyor-tool.ps1&quot; Import-Module &#39;..\\appveyor-tool.ps1&#39; install: ps: Bootstrap # Adapt as necessary starting from here before_build: ps: cd newpkg build_script: - ps: cd newpkg - travis-tool.sh install_deps test_script: - travis-tool.sh run_tests on_failure: - 7z a failure.zip *.Rcheck\\* - appveyor PushArtifact failure.zip artifacts: - path: &#39;*.Rcheck\\**\\*.log&#39; name: Logs - path: &#39;*.Rcheck\\**\\*.out&#39; name: Logs - path: &#39;*.Rcheck\\**\\*.fail&#39; name: Logs - path: &#39;*.Rcheck\\**\\*.Rout&#39; name: Logs - path: &#39;\\*_*.tar.gz&#39; name: Bits - path: &#39;\\*_*.zip&#39; name: Bits Here, we only run some tests and checks, since we already got the code analysis and coverage. Code coverage of our project yielded us the following results: Code coverage If we check the travis log, we see it has succeeded, because all tests ran through. However, we have some lints we should fix. $ tar -C .. -xf $PKG_TARBALL after_script.2 2.22s$ Rscript -e 'covr::codecov()' $message [1] \"Coverage reports upload successfully\" $id [1] \"552fd0db-187b-434e-b37b-a0e4fcba7636\" $meta $meta$status [1] 200 $queued [1] TRUE $url [1] \"https://codecov.io/github/dirmeier/essential-R/commit/f24d277296f78512524f26f3ca3b31d202e122a1\" $uploaded [1] TRUE after_script.3 1.23s$ Rscript -e 'lintr::lint_package()' R/bad_bad_file.R:3:1: style: lines should not be more than 80 characters. myRet = a+b ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ R/bad_bad_file.R:3:79: style: Variable and function names should be all lowercase. myRet = a+b ^~~~~ R/bad_bad_file.R:3:85: style: Use Fixing style related issues is essential since we want other people to be able to read our code easily. Good code also increases the number of users, because the package is more trusted than a spaghetti code package. Beauty is more important in computing than anywhere else in technology because software is so complicated. Beauty is the ultimate defense against complexity. – David Gelernter 9.2 Version control Aside from he fact that version control is great, putting your project on GitHub has the most prominent advantage that you can add badges to your README.md to show others about the state of your package, for instance repository, CI or code coverage status. There’s a wide variety of badges to describe your project. Description Badge Is the project passing on windows? How long is it on Bioconductor? Is it installable using conda? What is its version on CRAN? How often has it been downloaded? 9.3 Docker TODO (needed for debugging c++) 9.4 Code style I try to follow two general guidelines when writing code. These are primarily not my personal preferences, nut adopted form packages like data.table, lme4, Matrix or Bioconductor. Whatever you do, just be consistent. There seem to be a lot of different preference around. If I mainly write using S3 classes and functions I prefer writing code like this: my.var &lt;- &quot;2&quot; i.am.a.function &lt;- function(i) { sapply(seq(10), function(i) { i + 1 }) } plot.me &lt;- function(x, ...) plot.default(x) .i.am.private &lt;- &quot;2&quot; For S4 classes and functions I recommend using the Bioconductor style guide or how lme4 and Matrix do it: setMethod(&quot;camelCaps&quot;, signature = signature(iAmAList = &quot;list&quot;)) 9.5 Debugging C++ from R TODO "]
]
