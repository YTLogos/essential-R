[
["optimization.html", "6 Optimization 6.1 Convex optimization 6.2 Non-linear optimizaton", " 6 Optimization Much of statistical and machine learning boils down to function optimization (if you are not necessarily interested in Bayesian inefrence). In you donâ€™t want to implement an optimizer for yourself, the following couple of libraries might be of interest to you. 6.1 Convex optimization CVXR is a package for discplined convex optimization. As log as you can formulate your objective following the conventions from the package, CVXR automatically verifies convexity and chooses a solver. As an example, below we compare linear regression models solved using OLS and Huber loss: library(CVXR) ## ## Attaching package: &#39;CVXR&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## power library(lme4) ## Loading required package: Matrix x &lt;- rnorm(100) y &lt;- 1 + 2 * x + rnorm(100) beta &lt;- Variable(1) obj.ols &lt;- sum((y - x %*% beta)^2) result.ols &lt;- solve(Problem(Minimize(obj.ols))) obj.huber &lt;- sum(CVXR::huber(y - x %*% beta, 1)) result.huber &lt;- solve(Problem(Minimize(obj.huber))) beta.hat.ols &lt;- result.ols$getValue(beta) beta.hat.huber &lt;- result.huber$getValue(beta) beta.hat.ols ## [1] 2.007891 beta.hat.huber ## [1] 2.038758 TODO more examples and references 6.2 Non-linear optimizaton In cases, where your objective is not differentiable, you have box-constraints, or whatsoever nloptr is an excellent choice for function minimization. Following the example from above: library(nloptr) ols &lt;- function(x, m, n) { 0.5 * sum((m - n * x)^2) } huaba &lt;- function(x, m, n) { thresh &lt;- sum(abs(m - n * x)) if (thresh &lt;= 1) ols(x, m, n) else thresh - 0.5 } result.hat.ols &lt;- nloptr::nloptr( 1, ols, lb = -10, ub = 10, opts=list(&quot;algorithm&quot; = &quot;NLOPT_LN_SBPLX&quot;, maxeval=1000), m=y, n=x) result.hat.huaba &lt;- nloptr::nloptr( 1, huaba, lb = -10, ub = 10, opts=list(&quot;algorithm&quot; = &quot;NLOPT_LN_SBPLX&quot;, maxeval=1000), m=y, n=x) result.hat.ols$solution ## [1] 2.007919 result.hat.huaba$solution ## [1] 2.039429 TODO more examples and references "]
]
