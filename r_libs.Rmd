---
title: "R libraries"
---

Much of R's popularity is due to its fantastic ecosystem. R users benefit heavily from CRAN and Bioconductor packages created by its community. For data analysis or statistics R is a good choice for various reasons:

- high-level,
- easy to install packages,
- can be extended to C++ without needing knowledge of linking or C++ build-systems,
- probably the best plotting facility with ggplot, cowplot, hrbrthemes or ggsci,
- Bioconductor, machine learning and statistics libraries,
- small standard library.

There are a few things, however, R is not so great at:

- in general slow,
- not really suited for large projects,
- poor object orientation,
- inconsistent function names,
- superior IDEs for Java, Python, ...,
- very limited threading capabilities.

For stats and data analysis I find R still superior to Python, partly due to `Rmarkdown`, `shiny`, `ggplot2` and `Rcpp`.


# Extending to C++ using Rcpp

Using Rcpp (and RcppEigen, RcppArmadillo, Boost) you can easily your extend your code to C++. It not only nicely wraps the standard C API, but also let's you use standard matrix libraries such as Eigen and Armadillo or Boost.

If you have a project that's not a package you can for instance create a function like this:

```{r}  
  Rcpp::cppFunction('double sum(std::vector<double>& vec) {
      double sum = .0;
      for (std::vector<double>::size_type i = 0; i < vec.size(); ++i)
          sum += vec[i];
      return sum;
  }')

  sum(as.numeric(seq(10)))
```

Let's have a look at an Eigen example with sourceCpp:

```{bash}
  cat ./square.cpp
```

```{r}  
  library('RcppEigen')
  Rcpp::sourceCpp("./square.cpp")
  square(matrix(rnorm(10), 2))
```

Even for small matrices, the speed up is already substantial.

```{r}  
  m <- matrix(rnorm(100 * 100), 100)
  microbenchmark::microbenchmark(
    square(m),
    t(m) %*% m
  )
```

## Rcpp in a package

Usually, you would want to put your source code into a `cpp` file in the `src` folder. Then you can call C++ from an R function, for instance as described below.

1) Define a C++ source file and put it in `src`, like this:

```{c++, eval=FALSE}
  #include <Rcpp.h>
  // [[Rcpp::export]]
  Rcpp::List dostuff()
  {
    // some computations

    return Rcpp::List::create(
      Rcpp::Named("a") = ...
    );
  }
```

2) Add this to your `DESCRIPTION`:

<pre><code>Imports: Rcpp
LinkingTo: Rcpp</code></pre>

3) Add a comment `#' @useDynLib newpkg, .registration = TRUE` to the documentation of any function, better yet to the package doc.

4) Call `devtools::document()`

5) Call `Rcpp::compileAttributes("./newpkg")`


This should let you access the C++ function from your package. For more details check out the main <a href="http://www.rcpp.org/">documentation</a> or some of my packages, like <a href="https://github.com/dirmeier/netReg">netReg</a> or <a href="https://github.com/dirmeier/datastructures">datastructures</a>. The latter also has an example how to use modules and Boost.


## Writing `C++`

Here are some tools that help you develop your code:

- gdb and lldb for debugging,
- valgrind and gprof to check for memory leaks and for profiling,
- Intel's Parallel Studio XE (<3) which is a toolbox for vectorization, debugging, memory checks, etc. If you can get hold of it, get it. It is magnificent.
- Boost for unit tests, data structures and basically everything you ever need,
- cppcheck for static code analysis,
- doxygen for code documentation.

Many C++ libraries, like Dlib, Eigen or tensorflow have an R interface, so before implementing functionality yourself check out if there is already an implementation for it.

At this point it also makes sense to have a look at various C++ books such as

- Scott Meyers: Effective Modern C++,
- Scott Meyers: Effective C++,
- Ben Klemens: 21. century C,
- Kurt Guntheroth: Optimized C++,
- Nicolai Josuttis: C++17 - the complete guide,
- David Vandevoorde: C++ Templates - the compete guide.

For fast numerical code OpenMP and advanced vector and streaming SIMD extensions (AVX/SSE) is often the way to go. Writing good code using AVX is however not very simple and knowledge of memory alignment is required. However, starting from version 4 (?), OpenMP easily allows vectorization using `#pragma simd`.

**Note: the last lines in this section are heavily subjective and only state the author's opinion.** *More and more garbage code is forced onto the scientific community, mostly related to academic hubris and lack of knowledge about how to *program* (we don't mean how to *accomplish a task using a programming language*, but to embrace a language and its philosophy at its very core). If you aim to publish your code, make sure it follows *contemporary* good practices, standards and guidelines and that it is consistent, for instance by following people's code who are part of the community for ages. At this point we want to emphasize that the author doesn't claim to know C++, but rather that he is annoyed by this very trend.*

C++ is a large, complex language. Chances that you mess up, introduce bugs and memory leaks are high, unless you have some experience. In academia in mostly results in bad, un-maintainable code. If your argument is *but it is faster than R*, at least have a look at Fortran, Julia or Python (and `numpy`). If you still prefer hacking in C++ try to stick to some guidelines:

- don't include C headers in your C++ projects .. just don't. C and C++ are **two** languages. There is for need for `FILE*` or `malloc`.
- modern C++ rarely needs pointers. Use `std::vector` or `std::unique_ptr`. No need to manually release pointers is an advantage of itself.
- BOOST has (probably) everything you will ever need. Don't reinvent the wheel and learn to include libraries in your project.
- Learn how to use Cmake, Meson or autotools. You should **never** need to directly invoke a compiler or write a hard-coded Makefile.
- Only because you have a PhD or M. Sc. in CS does not make you a prolific programmer. It requires staying up to date, **reading other people's code** and embracing of new ideas and the very philosophy of the language and its community.
- Print warnings. They are usually there for a reason.
- Start a toy project or contribute to OSS projects.

# Functional programming

`purrr` is a functional programming toolkit, much like the `apply` class of functions. However, purrr does so in a more unified way with consistent return values. Furthermore, in combination with `magrittr`, the code you write is naturally more concise and easier to read.

```{r, eval=TRUE, message=TRUE, include=TRUE, echo=TRUE, warnings=FALSE, fig.align = 'center'}
  library('purrr')
  library('ggplot2')
  library('repurrrsive')
  library('magrittr')  
  utils::head(sw_people[[1]])
  df <- map_dfr(
    sw_people,
    .f = function(.) data.frame(color  = .[["eye_color"]],
                                height = .[["height"]],
                                mass   = .[["mass"]]))
  utils::head(df)
```

# Plotting

One of the most significant reasons to use R is probably its plotting capabilities and people contributing to ggplot2 and the community's effort of adding package that integrate with it. Some nice packages for plotting are the following:

- `hrbrthemes`
- `ggthemes`
- `cowplot`
- `ggpubr`
- `ggthemr`
- `ggsci`
- `viridis` and `viridisLite`
- `colorspace` and `colorblindr`
- `highcharter`
- `patchwork`
- `animation`
- `plotly`
- `lattice`
- `magick`
- `imager`
- `scales`
- `tweenr`
- `ggraph` and `ggnetwork`
- `gganimate`


## Layout

Use `hrbrthemes` for a nicer layout:

```{r, fig.align = 'center', fig.height=6}
  library('hrbrthemes')
  library('ggsci')
  import_roboto_condensed()

  g <- ggplot(df) +
    geom_point(aes(x=height, y=mass, color=color)) +
    hrbrthemes::theme_ipsum_rc() +
    ggsci::scale_color_rickandmorty()
  print(g)
```

Recently I started changing to simpler layouts and themes, for instance as described on <a href="http://motioninsocial.com/tufte/">
Tufte in R</a>. `ggthemes` offers some nice options to do so. For instance, if you are feeling jealous that you cannot draft some Excel plots, cause you are working with R:

```{r, fig.align = 'center', fig.height=6}
  library('ggthemes')

  g <- ggplot(df) +
    geom_point(aes(x=height, y=mass), color="green") +
    ggthemes::theme_excel()
  print(g)
```


### Publication ready plots

`cowplot` and `ggpubr` are great for greating publication ready plots:

```{r, fig.align = 'center', fig.height=6}
  p1 <- ggplot(mtcars, aes(hp, disp)) + geom_point() + ggthemes::theme_tufte()
  p2 <- ggplot(mtcars, aes(hp, disp)) + geom_point() + ggthemes::theme_gdocs()

  cowplot::plot_grid(p1, p2, ncol=2, align="vh", labels=c("Nice", "Meh"))
```

## Colors

I use `colorspace`  and `colorblindr` in order to remove some hue and chroma. `viridis` is a wonderful set of colors for continuous, sequential data. For discrete, qualitative data I mainly use `ggthemr` and the `fresh` colors. `ggsci` also has some wonderful color palettes. `scales` lets you have a look at a color palette easily.

```{r, fig.align = 'center', fig.height=6}
  library('colorspace')
  library('colorblindr')
  library('viridis')
  library('ggthemr')
  library('scales')
  library('cowplot')
  ggthemr("fresh")

   p1 <- colorblindr::gg_color_gradient() + colorspace::scale_fill_continuous_sequential("viridis", c1=20, c2=70, l1=25, l2=100)
   p2 <- colorblindr::gg_color_gradient() + colorspace::scale_fill_continuous_sequential("Blues", c1=20, c2=70, l1=25, l2=100)
   p3 <- colorblindr::gg_color_gradient() +  colorspace::scale_fill_continuous_diverging(c1=40)
   df <- data.frame(Col=ggthemr::swatch()[1:10], X=1, Y=seq(10))
   p4 <- ggplot(df) + geom_tile(aes(x=Y, y=X), fill=df$Col) + theme_void()

   cowplot::plot_grid(p1, p2, p3, p4, ncol=2, align="vh")
```


## Interactive plots

Often you want to create an interactive plot, for instance when serving on a `shiny` instance. One way to do that is using plotly:

```{r, fig.align = 'center', fig.height=6}
  library('plotly')
  q <- qplot(data=iris, x=Sepal.Length, y=Sepal.Width, color=Species) +
    ggthemes::theme_tufte()
  plotly::ggplotly(q)
```

Recently I also stumbled upon `highcharter`:

```{r, fig.align = 'center', fig.height=6}
  library('highcharter')
  hchart(iris, "scatter", hcaes(x = Sepal.Length, y = Sepal.Width,  group=Species)) %>%
    hc_add_theme(hc_theme_tufte())
```



# data.table

`data.table` is a fast implementation of R's classical `data.frame`. I hardly ever use data frame any more, and if so only, because it seems to work nicer with `dplyr` and `tidyr`. However, using `dtplyr` this isn't much of a problem, really.

```{r, fig.width=2, fig.height=2, fig.align = 'center'}
  library('data.table')  
  library('dplyr')
  library('dtplyr')
  library('grid')
  library('gridExtra')

  n    <- 1000
  rn   <- stats::rnorm(n)
  ltrs <- base::sample(letters[1:5], n, replace=TRUE)
  dt   <- data.table::data.table(X=rn, Y=ltrs)
  df   <- base::data.frame(X=rn, Y=ltrs)

  dt[, .SD[sample(.N, 1)], by=c("Y")] %>%
    tableGrob(rows=NULL) %>%
    grid.arrange
```

In the end it depends what style you prefer. I usually go with `data.table` alone without needing the `dplyr/dtplyr` dependency. However, the latter is usually more readable. For large data, the fastest solution is probably preferable.

```{r}
  dt.only   <- function() dt[, .SD[sample(.N, 1)], by=c("Y")]
  dt.dtplyr <- function() dt %>% dplyr::group_by(Y) %>% dplyr::sample_n(1)
  df.dplyr  <- function() df %>% dplyr::group_by(Y) %>% dplyr::sample_n(1)

  microbenchmark::microbenchmark(
    dt.only(),
    dt.dtplyr(),
    df.dplyr()
  )
```


# mlR and openML

CRAN offers dozens of packages related to machine and statistical learning, many of which doing the same. `mlR` wraps many of these into one big library.
`mlR` integrates with `openML`, an open machine learning platform where people share code, data and algorithms. Here we show an example where we use a Gaussian process to predict the Kyphosis label from the `gam` package.

```{r}
  library(mlr)
  task  <- mlr::makeClassifTask(data = kyphosis, target = "Kyphosis")
  lrn   <- mlr::makeLearner("classif.gausspr")

  n <- nrow(kyphosis)
  train.set <- sample(n, size = 2/3*n)
  test.set  <- setdiff(1:n, train.set)

  model <- mlr::train(lrn, task, subset = train.set)
  pred  <- stats::predict(model, task = task, subset = test.set)
  performance(pred, measures = list(mmce, acc))
```


# Markdown and Shiny

Rmarkdown is a great way for documentation, reporting and working reproducibly, for instance by building interactive web-sites using `shiny`, static pages using HTML documents using the Bootstrap framework or plain handouts using Tufte style. For presentations that use R code I use <a href="https://github.com/yihui/xaringan">xaringan</a>.

### Shiny

Next to Rmarkdown, setting up a Shiny instance for reporting is a nice way to present data with interactive plots. Setting up shiny is fairly easy. You need to create a `server.R` file and `ui.R` file, e.g. like this:

```{bash}
  cat R/ui.R
```

```{bash}
  cat R/ui.R
```

You can publish your server on <a href="http://www.shinyapps.io/">shinyapps.io</a> so that it is accessible by everyone.

# Pryr

In some cases it is interesting to have a look at the implementation of specific functions, get their sizes in byte or get the current memory consumption. It is especially useful to keep track of the addresses your objects point to. When I was new to R I found it confusing when a reference is dropped and when a new copy of an object is created.

```{r}
  library('pryr')
  pryr::inspect(list())
  pryr::inspect(vector())

  pryr::object_size(numeric())
  pryr::object_size(numeric(1))

  x <- stats::rnorm(10)
  y <- x
  pryr::address(x)
  pryr::address(y)
  y[1] <- 1
  pryr::address(y)
```

# datastructures

If you have a background in computer science you may wonder, that R does not have support for advanced data structures such as Fibonacci heaps or hashmaps.
`datastructures` tries to solve this. It uses Rcpp modules to export Boost data structures to R:

```{r}
  library('datastructures')
  q <- datastructures::fibonacci_heap("integer", "numeric")
  q[1:3] <- list(rnorm(3), 2, rnorm(4))
  datastructures::pop(q)
  datastructures::pop(q)
  datastructures::pop(q)
  datastructures::pop(q)
```

# Other

There are many other great tools I did not specifically mention due to the fact that they are more situational. Some of these are:

- dplyr,
- tidyr,
- optparse,
- IRKernel,
- minqa and nloptr,
- glmnet, lme4 and netReg.
