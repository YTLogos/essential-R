# Data analysis and statistical modeling

The following few sections introduce packages I frequently use for data analysis, modelling and machine learning.

## Stan

`Stan` is a great package for Bayesian modelling and inference. Compared to statistical packages that are fit to one model, such as `glmnet` or `lme4`, Stan allows to easily define custom Bayesian models for which posterior distributions are automatically inferred using HMC. 

A simple liner regression model could look like this:
```{r}
library(rstan)
library(lme4)

model <- "
data {
  int<lower=1> n;
  vector[n] x;
  vector[n] y;
}

parameters {
  real beta;
  real<lower=0> sigma;
  real alpha;
}

model {
  beta ~ normal(0, 5);
  sigma ~ cauchy(0, 5);
  y ~ normal(alpha + x * beta, sigma);
}
"

n <- nrow(sleepstudy)
x <- sleepstudy$Days
y <- sleepstudy$Reaction

fit <- stan(model_code=model, data=list(n=n, x=x, y=y), warmup=100, iter=1100, chains=1)
summary(fit)
```

### Rstanarm

`rstanarm` is a package for applied Bayesian modelling that wraps around Stan for easier usage. 

```{r}
library(rstanarm)

fit <- rstanarm::stan_glm(Reaction ~ Days, sleepstudy, chains = 1,
                          iter = 1100, warmup = 100, family = gaussian())
summary(fit)
```

### brms

For non-linear multi-level models `brms` is also a great option. 

```{r}
library(brms)

fit.multilevel <- brm(Reaction ~ Days + (Days | Subject), sleepstudy, chains=1, iter=1000)
summary(fit.multilevel)
```

### bayesplot

For plotting of Bayesian models inferred using the tools mentioned above, you probably want to use `bayesplot`. For instance, to compare posterior predictive intervals between the `rstanarm` linear model and the mixed model form `brms`:

```{r, fig.width=10, fig.height=4}
library(bayesplot)
library(cowplot)

p1 <- ppc_intervals(y = sleepstudy$Reaction,
              yrep = posterior_predict(fit)) +
  labs(title="Linear model")
p2 <- ppc_intervals(y = sleepstudy$Reaction,
                    yrep = posterior_predict(fit.multilevel)) +
  labs(title="Linear mixed model")

cowplot::plot_grid(p1, p2, ncol=2, align="h")
```

## greta

Model definitions in Stan are arguable difficult in the beginning. As an alternative with `greta` is not only easier to compose models, but also often faster owing to the fact that it's developed against tensorflow.

The same model we used for stan above, looks like this with greta:
```{r}
library(greta)
library(lme4)

n <- nrow(sleepstudy)
x <- sleepstudy$Days
y <- sleepstudy$Reaction

alpha <- variable()
beta <- greta::normal(0, 5)
sigma <- greta::cauchy(0, 5, truncation=c(0, Inf))

y <- greta::as_data(y)
greta::distribution(y) <- greta::normal(alpha + x * beta, sigma)

mod <- greta::model(beta)
samples <- greta::mcmc(mod, n_samples=1000, warmup=100, chains=1)
summary(samples)
```

## MCMC

There are a couple of packages on CRAN especially for Markov Chain Monte Carlo and Bayesian methods, some of which are mentioned below.

`coda` is a package for analysis and diagnostics of MCMC chains. Mostly it takes arguments of class mcmc.list, so put your results into an object of it to be able to use `coda`:
```{r}
library(coda)

coda::autocorr(samples)
coda::traceplot(samples)
```

`MCMCpack` offer an alternative to rstanarm for applied Bayesian modelling. It also comes with a set of useful distributions used frequently in Bayesian modelling, such as the Dirichlet, inverse gamma, etc. 

```{r}
library(MCMCpack)
invg <- MCMCpack::rinvgamma(1000, 5, 1)
hist(invg, breaks=50, xlab="X", main="", col="darkgrey", family="serif")
```

For plotting `mcmc.list` objects (from `coda`) `MCMCvis` is great:

```{r}
MCMCvis::MCMCplot(samples)
```

## mlR and openML

CRAN offers dozens of packages related to machine and statistical learning, many of which doing the same. `mlR` wraps many of these into one big library.
`mlR` integrates with `openML`, an open machine learning platform where people share code, data and algorithms. Here we show an example where we use a Gaussian process to predict the Kyphosis label from the `gam` package.

```{r}
library(mlr)
task  <- mlr::makeClassifTask(data = kyphosis, target = "Kyphosis")
lrn   <- mlr::makeLearner("classif.gausspr")

n <- nrow(kyphosis)
train.set <- sample(n, size = 2/3*n)
test.set  <- setdiff(1:n, train.set)

model <- mlr::train(lrn, task, subset = train.set)
pred  <- stats::predict(model, task = task, subset = test.set)
performance(pred, measures = list(mmce, acc))
```

## Tensorflow

Thanks to Rstudio, R users are able to use `tensorflow`, a library for high performance numerical computations (which for instance greta uses). For instance, a linear model could look like this:

```{r}
library(tensorflow)

n <- nrow(sleepstudy)
x <- sleepstudy$Days
y <- sleepstudy$Reaction

# define model
beta  <- tf$Variable(tf$random_normal(shape(1L), 0, 10))
alpha <- tf$Variable(tf$zeros(shape(1L)))
y.hat <- alpha + beta * x

# Minimize the mean squared errors.
loss <- tf$reduce_mean((y - y.hat) ^ 2)
optimizer <- tf$train$GradientDescentOptimizer(.5)
train <- optimizer$minimize(loss)

# Launch the graph and initialize the variables.
sess <- tf$Session()
sess$run(tf$global_variables_initializer())

for (step in 1:100)
{
  sess$run(train)
}
cat(step, "-", sess$run(alpha), sess$run(beta), "\n")
```

## Keras

`Keras` is an interface to popular numerical libraries such as tensorflow and theano for which model/network definitions are independent of the library on the backend.
Our tensorflow model from above would look the following in keras:

```{r}
library(keras)

model <- keras_model_sequential() %>%
  layer_dense(units = 1, activation = "linear", input_shape = 1)
model %>%
  compile(loss = "mse",
          optimizer = optimizer_sgd(0.5),
          metrics = list("mean_absolute_error"))

summary(model)
```

## Regression models and machine learning

Some packages for regression:

* `glmnet` for $\ell_1$- and $\ell_2$-penalized linear regression models,
* `lme4` for frequentist mixed models,
* `mgcv` for generalized additive models,
* `netReg` for graph regularized linear models,
* `xgboost` and `gbm` for boosting.
* `h20` for general machine learning algorithms,

## Big data analytics

For Big data analytics I recommend Rstudio's `sparklyr` since it nicelt itegrates with the other methods from the `tidyverse`. For instance, following an example from [Rstudio's tutorials](https://spark.rstudio.com/mlib/):

```{r, eval=FALSE}
library(sparklyr)

sc <- spark_connect(master = "local")
kmeans_model <- copy_to(sc, iris, "iris", overwrite = TRUE) %>%
  select(Petal_Width, Petal_Length) %>%
  ml_kmeans(centers = 3)git add -A
```

## Others

Some other great packages for various data-related things:

### modelr

`modelr` defines multiple helper functions related to statistical modelling:

```{r}
library(modelr)

sleepstudy %>% modelr::fit_with(
  lm, 
  modelr::formulas(~Reaction,
                   no_intercept = ~0 + Days, 
                   intercept = ~1 + Days))
```

### kernlab

Use `kernlab` for tasks related to kernels, such as Gaussian process regression or merely computing a Gram-matrix.

```{r}
library(kernlab)

x <- matrix(rnorm(25), 5)
rbf <- kernlab::rbfdot()
(K <- kernlab::kernelMatrix(rbf, x))

x <- sort(rnorm(100))
y <- 3 + 0.5 * x^2 + 1 * x + rnorm(100, 0, 0.05)
gpr <- kernlab::gausspr(x, y)
plot(x, y, family="serif", a)
lines(x, predict(gpr, x), col="red")
```
